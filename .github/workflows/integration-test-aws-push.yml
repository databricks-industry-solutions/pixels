name: AWS integration test push

on:
 workflow_dispatch:
 push:
    branches:
      - main

jobs:
 run-databricks-notebook:
   runs-on: html_publisher
   steps:
     - name: Checkout repo
       uses: actions/checkout@v2
     - name: Run a databricks notebook
       uses: databricks/run-notebook@v0
       with:
         local-notebook-path: RUNME.py
         git-commit: ${{ github.sha }}
         databricks-host: https://e2-demo-west.cloud.databricks.com
         databricks-token: ${{ secrets.DEPLOYMENT_TARGET_TOKEN_AWS }}
         new-cluster-json: >
           {
             "num_workers": 0,
             "spark_version": "13.3.x-scala2.12",
             "node_type_id": "i3.xlarge",
             "aws_attributes": {
               "availability": "ON_DEMAND"
             },
             "spark_conf": {
                  "spark.master": "local[*, 4]",
                  "spark.databricks.cluster.profile": "singleNode"
              },
              "custom_tags": {
                  "ResourceClass": "SingleNode"
              },
              "policy_id": "E0631F5C0D0006EA"
           }
         notebook-params-json: >
           {
            "run_job": "True"
           }
         access-control-list-json: >
           [
             {
               "group_name": "users",
               "permission_level": "CAN_VIEW"
             }
           ]
