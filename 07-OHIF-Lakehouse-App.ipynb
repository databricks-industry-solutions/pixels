{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8c3217",
   "metadata": {},
   "source": [
    "Databricks notebook source\n",
    "MAGIC %md\n",
    "MAGIC ### Deploying OHIF Viewer in a Serverless Lakehouse App\n",
    "MAGIC\n",
    "MAGIC This notebook guides you through the process of deploying the OHIF Viewer as a serverless lakehouse application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c0719",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92e329",
   "metadata": {},
   "source": [
    "MAGIC %pip install --upgrade databricks-sdk==0.88.0 psycopg[binary,pool] psycopg2-binary fsspec -q\n",
    "MAGIC dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c4af1",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23aa9f",
   "metadata": {},
   "source": [
    "MAGIC %run ./config/proxy_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d5819",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a249b",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Initializing Environment and Setting Up Application\n",
    "MAGIC\n",
    "MAGIC Initialize widgets to capture the SQL warehouse ID, table, and volume. We also set up the environment and define the application name as \"pixels-ohif-viewer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519786a",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ee5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_warehouse_id, table, volume = init_widgets(show_volume=True)\n",
    "init_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ea4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"pixels-dicomweb\"\n",
    "lakebase_instance_name = \"pixels-lakebase\"\n",
    "serving_endpoint_name = \"pixels-monai-uc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f237b98d",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2347f0",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Setting Up and Deploying the Lakehouse Application\n",
    "MAGIC\n",
    "MAGIC The next step will perform several critical steps to set up and deploy our Lakehouse Application:\n",
    "MAGIC\n",
    "MAGIC 1. **Import Necessary Libraries**: We start by importing required libraries and modules such as `AppResource`, `AppResourceSqlWarehouse`, and others from the `databricks.sdk.service.apps`, along with `Path` from `pathlib`, and `dbx.pixels.resources`.\n",
    "MAGIC\n",
    "MAGIC 2. **Initialize Workspace Client**: An instance of `WorkspaceClient` is created to interact with the Databricks workspace.\n",
    "MAGIC\n",
    "MAGIC 3. **Prepare Application Configuration**: The application's configuration is prepared by reading a template configuration file (`app-config.yaml`), replacing placeholders with actual values (like the pixels table name), and writing the modified configuration to `app.yaml`.\n",
    "MAGIC\n",
    "MAGIC 4. **Define SQL Warehouse Resource**: We define a `sql_resource` with the SQL warehouse ID and permissions required for the application to use the SQL warehouse.\n",
    "MAGIC\n",
    "MAGIC 5. **Create and Deploy the Application**: The application is created and deployed using the `create_and_wait` and `deploy_and_wait` methods of the `WorkspaceClient`. This process involves specifying the application name, resources (like the SQL warehouse resource), and the path to the application's source code.\n",
    "MAGIC\n",
    "MAGIC 6. **Extract Service Principal ID**: After deployment, the service principal ID is extracted from the deployment artifacts for permission grants.\n",
    "MAGIC\n",
    "MAGIC 7. **Output Deployment Status and URL**: Finally, the deployment status message and the application URL are printed, indicating the completion of the deployment process and how to access the deployed application.\n",
    "MAGIC\n",
    "MAGIC This cell encapsulates the entire process of preparing, creating, and deploying the Lakehouse Application, making it a pivotal step in the application setup workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52586e1c",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd321936",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Create Lakebase DB and DICOM_FRAMES table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5d3f6",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb66bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dbx\n",
    "from dbx.pixels.lakebase import LakebaseUtils\n",
    "lb_utils = LakebaseUtils(instance_name=lakebase_instance_name, uc_table_name=table, create_instance=True, min_cu=0.5, max_cu=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81758c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(dbx.pixels.__file__)\n",
    "sql_base_path = f\"{path}/resources/sql/lakebase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database is aligned to UC catalog, schema to UC schema\n",
    "# (e.g. catalog.schema.table → database=catalog, schema=schema)\n",
    "_lb_schema = lb_utils.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c81e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sql_file in [\"CREATE_LAKEBASE_SCHEMA.sql\", \"CREATE_LAKEBASE_DICOM_FRAMES.sql\"]:\n",
    "    file_path = os.path.join(sql_base_path, sql_file)\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lb_utils.execute_query(file.read().format(schema_name=_lb_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the UC view used by Reverse ETL to sync instance_paths into Lakebase\n",
    "from dbx.pixels.lakebase import parse_uc_table_name\n",
    "_uc_catalog, _uc_schema, _uc_table = parse_uc_table_name(table)\n",
    "with open(os.path.join(sql_base_path, \"CREATE_INSTANCE_PATHS_VIEW.sql\"), \"r\") as file:\n",
    "    spark.sql(file.read().format(catalog=_uc_catalog, schema=_uc_schema, table=_uc_table))\n",
    "print(f\"✓ Created UC view {_uc_catalog}.{_uc_schema}.instance_paths_vw for Reverse ETL Sync\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6d26e",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc53def",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Create Reverse ETL Synced Table\n",
    "MAGIC\n",
    "MAGIC Sync the `instance_paths_vw` view from Unity Catalog into Lakebase using\n",
    "MAGIC Reverse ETL.  This creates:\n",
    "MAGIC\n",
    "MAGIC 1. A **read-only synced table** in UC (`catalog.schema.instance_paths`)\n",
    "MAGIC 2. A **Postgres table** in Lakebase (`schema.instance_paths`)\n",
    "MAGIC\n",
    "MAGIC The sync pipeline keeps the Lakebase table continuously updated so the\n",
    "MAGIC DICOMweb app can resolve SOP Instance UIDs → file paths in sub-10 ms\n",
    "MAGIC without querying the SQL warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1bbb9",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.service.database import (\n",
    "    SyncedDatabaseTable,\n",
    "    SyncedTableSpec,\n",
    "    NewPipelineSpec,\n",
    "    SyncedTableSchedulingPolicy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "_synced_table_name = f\"{_uc_catalog}.{_uc_schema}.instance_paths\"\n",
    "_source_view_name  = f\"{_uc_catalog}.{_uc_schema}.instance_paths_vw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ff0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the synced table already exists\n",
    "try:\n",
    "    existing = w.database.get_synced_database_table(name=_synced_table_name)\n",
    "    print(f\"Synced table '{_synced_table_name}' already exists — state: {existing.data_synchronization_status.detailed_state}\")\n",
    "except Exception:\n",
    "    synced_table = w.database.create_synced_database_table(\n",
    "        SyncedDatabaseTable(\n",
    "            name=_synced_table_name,\n",
    "            database_instance_name=lakebase_instance_name,\n",
    "            logical_database_name=_uc_catalog,\n",
    "            spec=SyncedTableSpec(\n",
    "                source_table_full_name=_source_view_name,\n",
    "                primary_key_columns=[\"local_path\"],\n",
    "                scheduling_policy=SyncedTableSchedulingPolicy.SNAPSHOT,\n",
    "                new_pipeline_spec=NewPipelineSpec(\n",
    "                    storage_catalog=_uc_catalog,\n",
    "                    storage_schema=_uc_schema,\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    print(f\"✓ Created synced table: {synced_table.name}\")\n",
    "    print(f\"  Source:   {_source_view_name}\")\n",
    "    print(f\"  Lakebase: {_uc_schema}.instance_paths\")\n",
    "    print(f\"  Mode:     SNAPSHOT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25199934",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67677f",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Deploy DICOMweb App\n",
    "MAGIC\n",
    "MAGIC Create (or update) the DICOMweb Databricks App.  The app-config template\n",
    "MAGIC is filled with the environment-specific values and written as `app.yml`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066c141",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.service.apps import (\n",
    "    AppResource,\n",
    "    AppResourceSqlWarehouse,\n",
    "    AppResourceSqlWarehouseSqlWarehousePermission,\n",
    "    AppResourceServingEndpoint,\n",
    "    AppResourceServingEndpointServingEndpointPermission,\n",
    "    App,\n",
    "    AppDeployment,\n",
    ")\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cf5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pixels_path = Path(dbx.pixels.__file__).parent\n",
    "_dicomweb_path = str(_pixels_path / \"resources\" / \"dicom_web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c059ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive STOW volume path from the volume widget (catalog.schema.volume_name)\n",
    "_vol_parts = volume.split(\".\")\n",
    "_stow_volume_path = f\"/Volumes/{_vol_parts[0]}/{_vol_parts[1]}/{_vol_parts[2]}/stow/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a86a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate app.yml from the template\n",
    "with open(f\"{_dicomweb_path}/app-config.yml\", \"r\") as config_input:\n",
    "    with open(f\"{_dicomweb_path}/app.yml\", \"w\") as config_output:\n",
    "        config_output.write(\n",
    "            config_input.read()\n",
    "            .replace(\"{PIXELS_TABLE}\", table)\n",
    "            .replace(\"{LAKEBASE_INSTANCE_NAME}\", lakebase_instance_name)\n",
    "            .replace(\"{LAKEBASE_INIT_DB}\", \"false\")\n",
    "            .replace(\"{LAKEBASE_RLS_ENABLED}\", \"false\")\n",
    "            .replace(\"{DICOMWEB_USE_USER_AUTH}\", \"false\")\n",
    "            .replace(\"{STOW_VOLUME_PATH}\", _stow_volume_path)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"✓ Generated app.yml for DICOMweb app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build app resources\n",
    "resources = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_resource = AppResource(\n",
    "    name=\"sql_warehouse\",\n",
    "    sql_warehouse=AppResourceSqlWarehouse(\n",
    "        id=sql_warehouse_id,\n",
    "        permission=AppResourceSqlWarehouseSqlWarehousePermission.CAN_USE,\n",
    "    ),\n",
    ")\n",
    "resources.append(sql_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "if serving_endpoint_name in [ep.name for ep in w.serving_endpoints.list()]:\n",
    "    resources.append(\n",
    "        AppResource(\n",
    "            name=\"serving_endpoint\",\n",
    "            serving_endpoint=AppResourceServingEndpoint(\n",
    "                name=serving_endpoint_name,\n",
    "                permission=AppResourceServingEndpointServingEndpointPermission.CAN_QUERY,\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or retrieve existing app\n",
    "if app_name in [a.name for a in w.apps.list()]:\n",
    "    print(f\"App '{app_name}' already exists\")\n",
    "    app = w.apps.get(app_name)\n",
    "else:\n",
    "    print(f\"Creating DICOMweb App '{app_name}' — this may take a few minutes …\")\n",
    "    app = App(\n",
    "        app_name,\n",
    "        default_source_code_path=_dicomweb_path,\n",
    "        user_api_scopes=[\"sql\", \"files.files\"],\n",
    "        resources=resources,\n",
    "    )\n",
    "    app = w.apps.create_and_wait(app)\n",
    "    print(f\"✓ App created: {app.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053874cb",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084f1bf",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Granting Permissions\n",
    "MAGIC\n",
    "MAGIC Grant the DICOMweb app's service principal access to:\n",
    "MAGIC - Lakebase tables (`dicom_frames`, `instance_paths`)\n",
    "MAGIC - Unity Catalog (catalog, schema, table, volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c74531",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_instance = w.apps.get(app_name)\n",
    "app_instance.service_principal_client_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee23276",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeabba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.service import catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_instance = w.apps.get(app_name)\n",
    "service_principal_id = app_instance.service_principal_client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3609cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakebase grants\n",
    "role = lb_utils.get_or_create_sp_role(service_principal_id)\n",
    "_role_name = role.spec.postgres_role\n",
    "lb_utils.execute_query(f'GRANT SELECT, INSERT ON {_lb_schema}.dicom_frames TO \"{_role_name}\"')\n",
    "lb_utils.execute_query(f'GRANT SELECT, INSERT ON {_lb_schema}.instance_paths TO \"{_role_name}\"')\n",
    "lb_utils.execute_query(f'GRANT USAGE ON SCHEMA {_lb_schema} TO \"{_role_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66096e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index for fast Study Instance UID lookups on the Reverse-ETL-synced table\n",
    "lb_utils.execute_query(f'CREATE INDEX IF NOT EXISTS idx_instance_paths_study ON {_lb_schema}.instance_paths (study_instance_uid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UC grants\n",
    "w.grants.update(\n",
    "    full_name=_uc_catalog,\n",
    "    securable_type=\"catalog\",\n",
    "    changes=[catalog.PermissionsChange(add=[catalog.Privilege.USE_CATALOG], principal=service_principal_id)],\n",
    ")\n",
    "w.grants.update(\n",
    "    full_name=f\"{_uc_catalog}.{_uc_schema}\",\n",
    "    securable_type=\"schema\",\n",
    "    changes=[catalog.PermissionsChange(add=[catalog.Privilege.USE_SCHEMA], principal=service_principal_id)],\n",
    ")\n",
    "w.grants.update(\n",
    "    full_name=table,\n",
    "    securable_type=\"table\",\n",
    "    changes=[catalog.PermissionsChange(add=[catalog.Privilege.ALL_PRIVILEGES], principal=service_principal_id)],\n",
    ")\n",
    "w.grants.update(\n",
    "    full_name=volume,\n",
    "    securable_type=\"volume\",\n",
    "    changes=[catalog.PermissionsChange(add=[catalog.Privilege.ALL_PRIVILEGES], principal=service_principal_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ Permissions granted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d1a57",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bdebf",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a8784",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c2ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_deploy = w.apps.deploy_and_wait(app_name, AppDeployment(source_code_path=_dicomweb_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"✓ {app_deploy.status.message}\")\n",
    "print(f\"  URL: {app.url}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
