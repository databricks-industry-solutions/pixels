{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66041fbb-18fb-431c-afbf-c6a5a9d0ec8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Binary PHI Flag\n",
    "The goal of this notebook is to assess the `.dcm` files associated with PHI by reverse engineering the outcome of PHI masking from the original files in order to create a binary flag of `requires_phi_masking` for the original data. With this flag, we can then assess performance metrics on various PHI detection solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c62f80e3-5aa3-42d8-9137-fec1e0e2ee3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../config/setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01332bcc-32e7-4492-89c9-3fc48c888c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import json\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "\n",
    "dicom_data = spark.read.table('hls_radiology.tcia.object_catalog')\n",
    "masked_data = spark.read.table('hls_radiology.tcia.midi_b_val_subset')\n",
    "joined_df = masked_data.join(dicom_data, \"path\")\n",
    "tag_df = spark.read.table(\"hls_radiology.ddsm.dicom_tags\")\n",
    "tag_mapping = dict(tag_df.select(\"Tag\", \"Keyword\").rdd.map(tuple).collect())\n",
    "tag_mapping_bc = spark.sparkContext.broadcast(tag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a5290d-cc34-4d2f-843e-138c633b44df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dicom_to_json(path: str) -> str:\n",
    "    local_path = path.replace(\"dbfs:\", \"\")\n",
    "    dcm = pydicom.dcmread(local_path)\n",
    "    return json.dumps(dcm.to_json_dict())\n",
    "\n",
    "dicom_to_json_udf = udf(dicom_to_json, StringType())\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def find_diff_tags_udf(meta_col: pd.Series, masked_col: pd.Series) -> pd.Series:\n",
    "    tag_mapping = tag_mapping_bc.value\n",
    "    results = []\n",
    "\n",
    "    for meta_str, masked_str in zip(meta_col, masked_col):\n",
    "        try:\n",
    "          meta_json = json.loads(meta_str or \"{}\")\n",
    "          masked_json = json.loads(masked_str or \"{}\")\n",
    "\n",
    "          diffs = []\n",
    "          for k, v in meta_json.items():\n",
    "              if k in masked_json:\n",
    "                  try:\n",
    "                      masked_value = masked_json[k].get(\"Value\")\n",
    "                      if masked_value != v.get(\"Value\"):\n",
    "                          diffs.append(tag_mapping.get(k, k))\n",
    "                  except AttributeError:\n",
    "                      continue\n",
    "          results.append(diffs)\n",
    "        except Exception:\n",
    "          results.append([])\n",
    "\n",
    "    return pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba6219cb-f1dd-4ff4-9898-35d0ccfd65bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_masked_json = (\n",
    "  joined_df\n",
    "    .withColumn(\"masked_json\", dicom_to_json_udf(\"path_masked\"))\n",
    "    .withColumn(\"diff_tags\", find_diff_tags_udf(\"meta\", \"masked_json\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754e935c-4c51-4a2b-8770-c6280718f815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_masked_data_diff = df_with_masked_json.toPandas()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_65bc13ea-276c-4905-a728-9fe2fb1780e2",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DICOM - Masked Metadata Diff",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
