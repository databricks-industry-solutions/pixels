{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8045869-bfca-42b0-bc02-74e9d5b002e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create STOW-RS Processor Job (Two-Phase)\n",
    "\n",
    "Creates a **serverless performance-optimized** Spark job that processes\n",
    "STOW-RS uploads in two phases:\n",
    "\n",
    "| Task | Notebook | Purpose |\n",
    "|---|---|---|\n",
    "| `stow_split` | `workflow/stow_split` | Phase 1 — split multipart bundles → individual DICOMs, MERGE `output_paths` \n",
    "| `stow_meta_extract` | `workflow/stow_meta_extract` | Phase 2 — `DicomMetaExtractor` → save to catalog (depends on 1) |\n",
    "\n",
    "The DICOMweb handler returns to the client **as soon as Phase 1 completes**\n",
    "(with the extracted file paths), while Phase 2 continues in the background.\n",
    "\n",
    "**Run this after** `07-OHIF-Lakehouse-App` has deployed the DICOMweb app.\n",
    "\n",
    "What this notebook does:\n",
    "1. Creates the `stow_operations` Delta table (if not exists)\n",
    "2. Creates the `<app_name>_stow_processor` job with two tasks (if not exists)\n",
    "3. Grants the app's service principal access to the STOW table and job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e3a6fcf-7773-4609-9a8c-5e03d180f466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade databricks-sdk==0.88.0 -q\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba534471-19bf-45dd-b6f4-f2139c80e20b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config/proxy_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cb963f5-6cd0-4f63-9a1b-9a395b898885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "604edec0-12c1-459d-b36e-16c5e4b0cb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_warehouse_id, table, volume = init_widgets(show_volume=True)\n",
    "init_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fc2f016-22de-4ab5-a404-7307362b1061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "app_name = \"pixels-dicomweb\"\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12073ff1-1688-4891-9b36-d9352527b547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Derive Names and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c943fb65-2c84-48bd-bdc5-529367444ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_parts = table.split(\".\")\n",
    "assert len(_parts) == 3, \"table must be catalog.schema.table\"\n",
    "_uc_catalog = _parts[0]\n",
    "_uc_schema = _parts[1]\n",
    "_uc_table = _parts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7360ce86-95f1-47c2-83c4-70d7c8730dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stow_table = f\"{_uc_catalog}.{_uc_schema}.stow_operations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7da3a927-c231-4b9a-b11d-8aa6f8e9fcac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Volume UC name (e.g. main.pixels_solacc.pixels_volume)\n",
    "_vol_parts = volume.split(\".\")\n",
    "assert len(_vol_parts) == 3, \"volume must be catalog.schema.volume_name\"\n",
    "volume_path = f\"/Volumes/{_vol_parts[0]}/{_vol_parts[1]}/{_vol_parts[2]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838fb396-8a0d-40c3-a426-3c148f65644e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Job name follows the convention used by the handler's _resolve_stow_job_id()\n",
    "job_name = f\"{app_name}_stow_processor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aaaea54-5de6-44b6-ad80-aa4e041a850f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook paths — derive from the current notebook's workspace location\n",
    "_notebook_path = (\n",
    "    dbutils.notebook.entry_point\n",
    "    .getDbutils().notebook().getContext()\n",
    "    .notebookPath().get()\n",
    ")\n",
    "_base_path = _notebook_path.rsplit(\"/\", 1)[0]\n",
    "stow_split_notebook = f\"{_base_path}/workflow/stow_split\"\n",
    "stow_meta_notebook = f\"{_base_path}/workflow/stow_meta_extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0a747c9-da59-48a6-aaa9-21d7af79f67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"App name              : {app_name}\")\n",
    "print(f\"Job name              : {job_name}\")\n",
    "print(f\"Catalog table         : {table}\")\n",
    "print(f\"STOW tracking table   : {stow_table}\")\n",
    "print(f\"Volume                : {volume}\")\n",
    "print(f\"Volume path           : {volume_path}\")\n",
    "print(f\"Phase 1 notebook      : {stow_split_notebook}\")\n",
    "print(f\"Phase 2 notebook      : {stow_meta_notebook}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217c3ce4-de14-436e-b388-a7b2597cc47e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create `stow_operations` Table\n",
    "The DDL is in `resources/sql/CREATE_STOW_OPERATIONS.sql`.\n",
    "`Catalog.init_tables()` also executes it, but we ensure it exists before\n",
    "creating the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40224a64-b784-4985-98a3-912d09d6f51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbx.pixels\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3df54b6f-cc3e-40f7-b3c6-f2b774452f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_sql_path = Path(dbx.pixels.__file__).parent / \"resources\" / \"sql\" / \"CREATE_STOW_OPERATIONS.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19f1950b-e127-401d-990c-fe3d35baa503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(_sql_path, \"r\") as f:\n",
    "    ddl = f.read().replace(\"{UC_SCHEMA}\", f\"{_uc_catalog}.{_uc_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6a23bcf-b406-4ada-a8cc-d259482a8889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(ddl)\n",
    "print(f\"✓ Table {stow_table} ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "229fb225-6b36-4dc7-92e4-8598c572e7fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Serverless Performance-Optimized Job (if not exists)\n",
    "\n",
    "The job has **two tasks**:\n",
    "\n",
    "| Task key | Notebook | Dependency |\n",
    "|---|---|---|\n",
    "| `stow_split` | `workflow/stow_split` | — |\n",
    "| `stow_meta_extract` | `workflow/stow_meta_extract` | `stow_split` |\n",
    "\n",
    "- Uses **serverless performance-optimized** compute (`disable_auto_optimization=False`)\n",
    "- Allows `max_concurrent_runs = 2` (1 running + 1 queued) for run coalescing\n",
    "- Default parameters match the widgets — the DICOMweb handler overrides\n",
    "  them via `job_parameters` in `run-now`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc1f185f-53e5-4ac4-84e0-18f0592df705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.jobs import (\n",
    "    Task,\n",
    "    TaskDependency,\n",
    "    NotebookTask,\n",
    "    Source,\n",
    "    JobEnvironment,\n",
    "    JobParameterDefinition,\n",
    "    JobAccessControlRequest,\n",
    "    JobPermissionLevel,\n",
    ")\n",
    "from databricks.sdk.service.compute import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4aecbbc-46a2-4f41-b48e-b1a67943f798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "existing_jobs = [j for j in w.jobs.list(name=job_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38fc3333-26fd-4984-9ed3-e54261259939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if existing_jobs:\n",
    "    job_id = existing_jobs[0].job_id\n",
    "    print(f\"Job '{job_name}' already exists (job_id: {job_id})\")\n",
    "else:\n",
    "    created = w.jobs.create(\n",
    "        name=job_name,\n",
    "        tasks=[\n",
    "            # Phase 1 — split multipart bundles → individual DICOMs\n",
    "            Task(\n",
    "                task_key=\"stow_split\",\n",
    "                notebook_task=NotebookTask(\n",
    "                    notebook_path=stow_split_notebook,\n",
    "                    source=Source.WORKSPACE,\n",
    "                ),\n",
    "                environment_key=\"default\",\n",
    "                disable_auto_optimization=False,  # serverless performance-optimized\n",
    "            ),\n",
    "            # Phase 2 — extract DICOM metadata → save to catalog\n",
    "            Task(\n",
    "                task_key=\"stow_meta_extract\",\n",
    "                notebook_task=NotebookTask(\n",
    "                    notebook_path=stow_meta_notebook,\n",
    "                    source=Source.WORKSPACE,\n",
    "                ),\n",
    "                depends_on=[TaskDependency(task_key=\"stow_split\")],\n",
    "                environment_key=\"default\",\n",
    "                disable_auto_optimization=False,  # serverless performance-optimized\n",
    "            ),\n",
    "        ],\n",
    "        environments=[\n",
    "            JobEnvironment(\n",
    "                environment_key=\"default\",\n",
    "                spec=Environment(\n",
    "                    client=\"4\",\n",
    "                    dependencies=[\n",
    "                        \"databricks-pixels @ git+https://github.com/databricks-industry-solutions/pixels@features/dicom_web_integration\",\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        max_concurrent_runs=1,\n",
    "        tags={\"app\": app_name, \"purpose\": \"stow_processor\"},\n",
    "        parameters=[\n",
    "            JobParameterDefinition(name=\"catalog_table\", default=table),\n",
    "            JobParameterDefinition(name=\"volume\", default=volume),\n",
    "        ],\n",
    "    )\n",
    "    job_id = created.job_id\n",
    "    print(f\"✓ Created job '{job_name}' (job_id: {job_id})\")\n",
    "    print(f\"  Task 1 (split)   : {stow_split_notebook}\")\n",
    "    print(f\"  Task 2 (meta)    : {stow_meta_notebook}\")\n",
    "    print(f\"  Compute          : serverless performance-optimized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e120fb2-7327-4852-b966-afc8b7f20148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Grant Permissions\n",
    "\n",
    "The DICOMweb app's service principal needs:\n",
    "- `ALL_PRIVILEGES` on `stow_operations` (INSERT from handler, MERGE from job)\n",
    "- `CAN_MANAGE_RUN` on the job (to trigger `run-now`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26c3a183-14f1-49cd-b46f-a8062bdee5a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Cell 13\n",
    "from databricks.sdk.service import catalog as catalog_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9575764e-c7cc-4f2c-8e3d-32683b99d8f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "app_instance = w.apps.get(app_name)\n",
    "service_principal_id = app_instance.service_principal_client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b08448cd-ed9c-4ec0-a104-c2547f6c6072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── UC grant on stow_operations table ─────────────────────────────────\n",
    "w.grants.update(\n",
    "    full_name=stow_table,\n",
    "    securable_type=\"table\",\n",
    "    changes=[\n",
    "        catalog_svc.PermissionsChange(\n",
    "            add=[catalog_svc.Privilege.ALL_PRIVILEGES],\n",
    "            principal=service_principal_id,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "print(f\"✓ Granted ALL_PRIVILEGES on {stow_table} to SP {service_principal_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ec0ebb-64cb-405f-9c9b-1bc7bb59467b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── Job permission — allow the app SP to trigger runs ──────────────────\n",
    "w.jobs.update_permissions(\n",
    "    job_id=str(job_id),\n",
    "    access_control_list=[\n",
    "        JobAccessControlRequest(\n",
    "            service_principal_name=service_principal_id,\n",
    "            permission_level=JobPermissionLevel.CAN_MANAGE_RUN,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "print(f\"✓ Granted CAN_MANAGE_RUN on job {job_id} to SP {service_principal_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e16e0b6-6a46-4a8d-bd01-ffe20afad830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Done\n",
    "\n",
    "The STOW-RS pipeline is ready:\n",
    "\n",
    "1. **DICOMweb app** streams uploads → temp file on Volumes + tracking row in `stow_operations`\n",
    "2. **App triggers** `run-now` on `<app_name>_stow_processor` (with run coalescing)\n",
    "3. **Task 1 (split)** reads new pending rows via CDF, splits multipart bundles, saves individual DICOMs, MERGEs `output_paths` back → **handler returns paths to client**\n",
    "4. **Task 2 (meta)** reads completed rows via CDF, applies `DicomMetaExtractor`, saves metadata to catalog\n",
    "\n",
    "To test manually:\n",
    "```\n",
    "w.jobs.run_now(job_id=<JOB_ID>, job_parameters={\"catalog_table\": \"<table>\", \"volume\": \"<volume>\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ab0b500-0cb0-4317-abd8-b1e70316de0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"✅ STOW-RS processor setup complete\")\n",
    "print(f\"   Job name : {job_name}\")\n",
    "print(f\"   Job ID   : {job_id}\")\n",
    "print(f\"   Table    : {stow_table}\")\n",
    "print(f\"   App SP   : {service_principal_id}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "07b-STOW-Processor-Job",
   "widgets": {}
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
