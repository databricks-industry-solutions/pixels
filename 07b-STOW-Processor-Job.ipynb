{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f91c7b",
   "metadata": {},
   "source": [
    "Databricks notebook source\n",
    "MAGIC %md\n",
    "MAGIC ### Create STOW-RS Processor Job (Two-Phase)\n",
    "MAGIC\n",
    "MAGIC Creates a **serverless performance-optimized** Spark job that processes\n",
    "MAGIC STOW-RS uploads in two phases:\n",
    "MAGIC\n",
    "MAGIC | Task | Notebook | Purpose |\n",
    "MAGIC |---|---|---|\n",
    "MAGIC | `stow_split` | `workflow/stow_split` | Phase 1 — split multipart bundles → individual DICOMs, MERGE `output_paths` back |\n",
    "MAGIC | `stow_meta_extract` | `workflow/stow_meta_extract` | Phase 2 — `DicomMetaExtractor` → save to catalog (depends on Phase 1) |\n",
    "MAGIC\n",
    "MAGIC The DICOMweb handler returns to the client **as soon as Phase 1 completes**\n",
    "MAGIC (with the extracted file paths), while Phase 2 continues in the background.\n",
    "MAGIC\n",
    "MAGIC **Run this after** `07-OHIF-Lakehouse-App` has deployed the DICOMweb app.\n",
    "MAGIC\n",
    "MAGIC What this notebook does:\n",
    "MAGIC 1. Creates the `stow_operations` Delta table (if not exists)\n",
    "MAGIC 2. Creates the `<app_name>_stow_processor` job with two tasks (if not exists)\n",
    "MAGIC 3. Grants the app's service principal access to the STOW table and job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f674f7",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d454a88a",
   "metadata": {},
   "source": [
    "MAGIC %pip install --upgrade databricks-sdk==0.88.0 -q\n",
    "MAGIC dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6cab10",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50acfd85",
   "metadata": {},
   "source": [
    "MAGIC %run ./config/proxy_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67af77",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448e187",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63c773",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_warehouse_id, table, volume = init_widgets(show_volume=True)\n",
    "init_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"pixels-dicomweb\"\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16468e2",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56291445",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Derive Names and Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d0db8",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2670b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "_parts = table.split(\".\")\n",
    "assert len(_parts) == 3, \"table must be catalog.schema.table\"\n",
    "_uc_catalog = _parts[0]\n",
    "_uc_schema = _parts[1]\n",
    "_uc_table = _parts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d844c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "stow_table = f\"{_uc_catalog}.{_uc_schema}.stow_operations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3774b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume UC name (e.g. main.pixels_solacc.pixels_volume)\n",
    "_vol_parts = volume.split(\".\")\n",
    "assert len(_vol_parts) == 3, \"volume must be catalog.schema.volume_name\"\n",
    "volume_path = f\"/Volumes/{_vol_parts[0]}/{_vol_parts[1]}/{_vol_parts[2]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b11c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job name follows the convention used by the handler's _resolve_stow_job_id()\n",
    "job_name = f\"{app_name}_stow_processor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook paths — derive from the current notebook's workspace location\n",
    "_notebook_path = (\n",
    "    dbutils.notebook.entry_point\n",
    "    .getDbutils().notebook().getContext()\n",
    "    .notebookPath().get()\n",
    ")\n",
    "_base_path = _notebook_path.rsplit(\"/\", 1)[0]\n",
    "stow_split_notebook = f\"{_base_path}/workflow/stow_split\"\n",
    "stow_meta_notebook = f\"{_base_path}/workflow/stow_meta_extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"App name              : {app_name}\")\n",
    "print(f\"Job name              : {job_name}\")\n",
    "print(f\"Catalog table         : {table}\")\n",
    "print(f\"STOW tracking table   : {stow_table}\")\n",
    "print(f\"Volume                : {volume}\")\n",
    "print(f\"Volume path           : {volume_path}\")\n",
    "print(f\"Phase 1 notebook      : {stow_split_notebook}\")\n",
    "print(f\"Phase 2 notebook      : {stow_meta_notebook}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34808902",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b290f451",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Create `stow_operations` Table\n",
    "MAGIC\n",
    "MAGIC The DDL is in `resources/sql/CREATE_STOW_OPERATIONS.sql`.\n",
    "MAGIC `Catalog.init_tables()` also executes it, but we ensure it exists before\n",
    "MAGIC creating the job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c40d1b",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7593839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dbx.pixels\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa99f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sql_path = Path(dbx.pixels.__file__).parent / \"resources\" / \"sql\" / \"CREATE_STOW_OPERATIONS.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(_sql_path, \"r\") as f:\n",
    "    ddl = f.read().replace(\"{UC_SCHEMA}\", f\"{_uc_catalog}.{_uc_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(ddl)\n",
    "print(f\"✓ Table {stow_table} ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c710c22",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0133c4ca",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Create Serverless Performance-Optimized Job (if not exists)\n",
    "MAGIC\n",
    "MAGIC The job has **two tasks**:\n",
    "MAGIC\n",
    "MAGIC | Task key | Notebook | Dependency |\n",
    "MAGIC |---|---|---|\n",
    "MAGIC | `stow_split` | `workflow/stow_split` | — |\n",
    "MAGIC | `stow_meta_extract` | `workflow/stow_meta_extract` | `stow_split` |\n",
    "MAGIC\n",
    "MAGIC - Uses **serverless performance-optimized** compute (`disable_auto_optimization=False`)\n",
    "MAGIC - Allows `max_concurrent_runs = 2` (1 running + 1 queued) for run coalescing\n",
    "MAGIC - Default parameters match the widgets — the DICOMweb handler overrides\n",
    "MAGIC   them via `job_parameters` in `run-now`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329a7db",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.service.jobs import (\n",
    "    Task,\n",
    "    TaskDependency,\n",
    "    NotebookTask,\n",
    "    Source,\n",
    "    JobEnvironment,\n",
    "    JobParameterDefinition,\n",
    "    JobAccessControlRequest,\n",
    "    JobPermissionLevel,\n",
    ")\n",
    "from databricks.sdk.service.compute import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_jobs = [j for j in w.jobs.list(name=job_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa804ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if existing_jobs:\n",
    "    job_id = existing_jobs[0].job_id\n",
    "    print(f\"Job '{job_name}' already exists (job_id: {job_id})\")\n",
    "else:\n",
    "    created = w.jobs.create(\n",
    "        name=job_name,\n",
    "        tasks=[\n",
    "            # Phase 1 — split multipart bundles → individual DICOMs\n",
    "            Task(\n",
    "                task_key=\"stow_split\",\n",
    "                notebook_task=NotebookTask(\n",
    "                    notebook_path=stow_split_notebook,\n",
    "                    source=Source.WORKSPACE,\n",
    "                ),\n",
    "                environment_key=\"default\",\n",
    "                disable_auto_optimization=False,  # serverless performance-optimized\n",
    "            ),\n",
    "            # Phase 2 — extract DICOM metadata → save to catalog\n",
    "            Task(\n",
    "                task_key=\"stow_meta_extract\",\n",
    "                notebook_task=NotebookTask(\n",
    "                    notebook_path=stow_meta_notebook,\n",
    "                    source=Source.WORKSPACE,\n",
    "                ),\n",
    "                depends_on=[TaskDependency(task_key=\"stow_split\")],\n",
    "                environment_key=\"default\",\n",
    "                disable_auto_optimization=False,  # serverless performance-optimized\n",
    "            ),\n",
    "        ],\n",
    "        environments=[\n",
    "            JobEnvironment(\n",
    "                environment_key=\"default\",\n",
    "                spec=Environment(\n",
    "                    client=\"4\",\n",
    "                    dependencies=[\n",
    "                        \"databricks-pixels @ git+https://github.com/databricks-industry-solutions/pixels@features/dicom_web_integration\",\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        max_concurrent_runs=1,\n",
    "        tags={\"app\": app_name, \"purpose\": \"stow_processor\"},\n",
    "        parameters=[\n",
    "            JobParameterDefinition(name=\"catalog_table\", default=table),\n",
    "            JobParameterDefinition(name=\"volume\", default=volume),\n",
    "        ],\n",
    "    )\n",
    "    job_id = created.job_id\n",
    "    print(f\"✓ Created job '{job_name}' (job_id: {job_id})\")\n",
    "    print(f\"  Task 1 (split)   : {stow_split_notebook}\")\n",
    "    print(f\"  Task 2 (meta)    : {stow_meta_notebook}\")\n",
    "    print(f\"  Compute          : serverless performance-optimized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e566f05",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668e6b9",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Grant Permissions\n",
    "MAGIC\n",
    "MAGIC The DICOMweb app's service principal needs:\n",
    "MAGIC - `ALL_PRIVILEGES` on `stow_operations` (INSERT from handler, MERGE from job)\n",
    "MAGIC - `CAN_MANAGE_RUN` on the job (to trigger `run-now`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16093c66",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ff1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Cell 13\n",
    "from databricks.sdk.service import catalog as catalog_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f41c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_instance = w.apps.get(app_name)\n",
    "service_principal_id = app_instance.service_principal_client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bc22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── UC grant on stow_operations table ─────────────────────────────────\n",
    "w.grants.update(\n",
    "    full_name=stow_table,\n",
    "    securable_type=\"table\",\n",
    "    changes=[\n",
    "        catalog_svc.PermissionsChange(\n",
    "            add=[catalog_svc.Privilege.ALL_PRIVILEGES],\n",
    "            principal=service_principal_id,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "print(f\"✓ Granted ALL_PRIVILEGES on {stow_table} to SP {service_principal_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Job permission — allow the app SP to trigger runs ──────────────────\n",
    "w.jobs.update_permissions(\n",
    "    job_id=str(job_id),\n",
    "    access_control_list=[\n",
    "        JobAccessControlRequest(\n",
    "            service_principal_name=service_principal_id,\n",
    "            permission_level=JobPermissionLevel.CAN_MANAGE_RUN,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "print(f\"✓ Granted CAN_MANAGE_RUN on job {job_id} to SP {service_principal_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20a5d54",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1911bac",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC # Done\n",
    "MAGIC\n",
    "MAGIC The STOW-RS pipeline is ready:\n",
    "MAGIC\n",
    "MAGIC 1. **DICOMweb app** streams uploads → temp file on Volumes + tracking row in `stow_operations`\n",
    "MAGIC 2. **App triggers** `run-now` on `<app_name>_stow_processor` (with run coalescing)\n",
    "MAGIC 3. **Task 1 (split)** reads new pending rows via CDF, splits multipart bundles, saves individual DICOMs, MERGEs `output_paths` back → **handler returns paths to client**\n",
    "MAGIC 4. **Task 2 (meta)** reads completed rows via CDF, applies `DicomMetaExtractor`, saves metadata to catalog\n",
    "MAGIC\n",
    "MAGIC To test manually:\n",
    "MAGIC ```\n",
    "MAGIC w.jobs.run_now(job_id=<JOB_ID>, job_parameters={\"catalog_table\": \"<table>\", \"volume\": \"<volume>\"})\n",
    "MAGIC ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae7885",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7903395",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"✅ STOW-RS processor setup complete\")\n",
    "print(f\"   Job name : {job_name}\")\n",
    "print(f\"   Job ID   : {job_id}\")\n",
    "print(f\"   Table    : {stow_table}\")\n",
    "print(f\"   App SP   : {service_principal_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a547cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
