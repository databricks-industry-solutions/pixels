{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f269332-ca64-4ece-a096-da2d95fbda54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You may find this solution accelerator at https://github.com/databricks-industry-solutions/pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43c5908a-9d00-49a5-b00d-1d0dd3091858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# De-identifying DICOM Data\n",
    "\n",
    "Since the image and metadata around DICOM imaging can contain sensitive information, you may want to think about ways to de-identify this data. But what sort of de-identification scheme should you adopt? Do you want pure anonymization, or two-way de-identification that allows anonymizing the data and then allowing for the data to be recreated in it's original form?\n",
    "\n",
    "Since the previous examples showed you how easy it is to get the data into Delta, let's now extend this capability using open source libraries to anonymize and/or de-identify the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd2a8002-6bdc-4a05-9bd8-5ccb69fb69f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Introducing Presidio\n",
    "\n",
    "Microsoft has been working on an open source library called Presidio (https://microsoft.github.io/presidio/) that provides this capability not just for tabular data, but for images as well. This library is built in Python. Before we begin, we need to make sure we install our dependancies. This starts with installing the specific Python packages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7802de-a403-4b71-b103-95c72073f4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install presidio_analyzer presidio_anonymizer presidio-structured presidio-image-redactor\n",
    "%pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "869b6a0d-485d-4dd6-bad1-bce0bc7f3ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We also need to install one Python-based NLP engine, in our case we'll use SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036d149f-fef0-4385-b191-03d74102ec66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a201f4-77a2-4e18-b019-76df7a122067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e21bafc3-5e11-47e3-ae5e-b943dcc7c35e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize the environment"
    }
   },
   "outputs": [],
   "source": [
    "%run ./config/setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "906de608-359e-45b8-b2b7-0dcb055ed57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path,table,volume,write_mode = init_widgets()\n",
    "\n",
    "catalog = f\"\"\"{table.split(\".\")[0]}\"\"\"\n",
    "schema = f\"\"\"{table.split(\".\")[0]}.{table.split(\".\")[1]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df3cbf8-52b3-4af8-8161-f16fcb45187c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from dbx.pixels import Catalog\n",
    "from dbx.pixels.dicom import DicomPlot, DicomMetaExtractor, DicomThumbnailExtractor\n",
    "from presidio_structured import StructuredEngine, PandasAnalysisBuilder, StructuredAnalysis, JsonAnalysisBuilder, JsonDataProcessor\n",
    "from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig, RecognizerResult\n",
    "\n",
    "import instance_counter_anonymizer\n",
    "\n",
    "from faker import Faker\n",
    "from json import loads, dumps\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1c96d1e-3448-4080-bc10-f3d98669c78b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Revisiting our test data\n",
    "\n",
    "Before we begin, let's re-look at our sample data from the starter notebooks provided in this repository. If we select our data from our table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc426b44-299b-4443-9522-ceddf6297e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from ${table}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4198a527-0101-4228-b6cb-f13830ad1812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    --rowid,\n",
    "    meta:['00100010'].Value[0].Alphabetic patient_name, \n",
    "    meta:['00082218'].Value[0]['00080104'].Value[0] `Anatomic Region Sequence Attribute decoded`,\n",
    "    meta:['0008103E'].Value[0] `Series Description Attribute`,\n",
    "    meta:['00081030'].Value[0] `Study Description Attribute`,\n",
    "    meta:`00540220`.Value[0].`00080104`.Value[0] `projection` -- backticks work for numeric keys\n",
    "FROM ${table}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c38c098f-336c-4199-aa2b-a588090f99a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We see that the metadata has a patient name in it; however, our test data is already anonymized. For demonstration purposes, we'll clone the table, and use the `faker` Python library to generate some random names for the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ace103-77f3-4241-b1b8-94be7948f9af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE ${table}_presidio DEEP CLONE ${table}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b15175-b444-489c-a399-8f45edec12c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker()\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "def generate_faker_name():\n",
    "  return fake.name()\n",
    "spark.udf.register(\"generate_faker_name\", generate_faker_name, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6417cdd3-3d97-485b-ba65-c385de8dba36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW patients_to_faker_names AS\n",
    "SELECT\n",
    "    --rowid,\n",
    "    row_number() over(order by meta:['00100010'].Value[0].Alphabetic) patient_row_id,\n",
    "    meta:['00100010'].Value[0].Alphabetic patient_name,\n",
    "    generate_faker_name() as fake_name\n",
    "FROM ${table}_presidio\n",
    "WHERE meta:['00100010'].Value[0].Alphabetic IS NOT NULL\n",
    "GROUP BY patient_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f987270f-514e-4f2f-b090-408bb6203a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from patients_to_faker_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a285a06-77b6-44ab-b6c9-cadd27e8f6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE INTO ${table}_presidio a\n",
    "USING patients_to_faker_names b ON meta:['00100010'].Value[0].Alphabetic = b.patient_name\n",
    "WHEN MATCHED THEN UPDATE SET meta = replace(meta, meta:['00100010'].Value[0].Alphabetic, b.fake_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730d31f6-0590-45ac-bb00-cc37a5fddf81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    --rowid,\n",
    "    meta:['00100010'].Value[0].Alphabetic patient_name, \n",
    "    meta:['00082218'].Value[0]['00080104'].Value[0] `Anatomic Region Sequence Attribute decoded`,\n",
    "    meta:['0008103E'].Value[0] `Series Description Attribute`,\n",
    "    meta:['00081030'].Value[0] `Study Description Attribute`,\n",
    "    meta:`00540220`.Value[0].`00080104`.Value[0] `projection` -- backticks work for numeric keys\n",
    "FROM ${table}_presidio\n",
    "WHERE meta:['00100010'].Value[0].Alphabetic is not null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25a6ce8c-db05-4aeb-83b0-6c1e15f9fd67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Anonymizing Data With Presidio and Pandas\n",
    "\n",
    "Presidio has several different Python interfaces for identifying and transforming on data, but one of the most straightforward methods in the library is the ability for it to work with dataframes directly in Pandas. Remember that in PySpark you can seamlessless move between a Spark dataframe and a Pandas dataframe with the simple `toPandas()` and `.createDataFrame()` methods. \n",
    "\n",
    "Let's start there: we'll read some data into a new dataframe, then convert it to a Pandas dataframe. First we'll load in the required libraries, then instantiate the `StructuredEngine()` of Presidio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0823a093-4d4b-49cd-9948-4296432b3651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_engine = StructuredEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35804784-dd65-49b8-b531-1eb4441da81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Then we'll get our data again, and use `toPandas()` to create a new Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771e84b6-7f66-48fd-8c1e-1c6088a67855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"SELECT\n",
    "    --rowid,\n",
    "    meta:['00100010'].Value[0].Alphabetic patient_name, \n",
    "    meta:['00082218'].Value[0]['00080104'].Value[0] `Anatomic Region Sequence Attribute decoded`,\n",
    "    meta:['0008103E'].Value[0] `Series Description Attribute`,\n",
    "    meta:['00081030'].Value[0] `Study Description Attribute`,\n",
    "    meta:`00540220`.Value[0].`00080104`.Value[0] `projection`\n",
    "    FROM {table}_presidio\n",
    "    WHERE meta:['00100010'].Value[0].Alphabetic is not null\n",
    "  \"\"\")\n",
    "\n",
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4c891c-5e12-45bc-ab53-dc3a40a3d658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3984f6ad-205e-445a-9011-d570acc6361d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now it's time to start anonymizing the data. Presidio has two major components that work together:\n",
    "\n",
    " * ***Analyzer***: The analyzer is responsible for sampling your data, whether it is a string, JSON, Pandas dataframe, or image, and identifying any potential sensitive data within what is provided. The output of the analyzer is a dictionary containing the field names or positions, and the type of sensitive data it *thinks* it is\n",
    " * ***Operator***: The operator is responsible for actually transforming your sensitive data, and can configured to simply replace values, encrypt or decrypt, or even call custom functions that you create to transform your data\n",
    "\n",
    "\n",
    "Let's start with doing some analysis on the dataframe. We can use the library and pass in our Pandas dataframe to let it produce the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10283a15-ebae-4529-91f1-db4a23cdf386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tabular_analysis = PandasAnalysisBuilder().generate_analysis(pandas_df, language = \"en\")\n",
    "tabular_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c598a517-c11a-4398-845d-7978ad2d4300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can see from the output that it thinks that the `patient_name` column is of type `PERSON` and that two other columns are also flagged as `PERSON`. Obviously, in this example, we only really want to anonymize the `patient_name` column, so we can \"override\" the analysis by manually providing the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ec6b9e-f33c-455b-9569-0125fdb44a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_analysis = StructuredAnalysis(entity_mapping={\n",
    "    \"patient_name\":\"PERSON\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a21b82dc-a8b1-4d62-ad1e-aee94dddf84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next we need to define our operator. Operators are defined by \"classification\"; meaning, for a specific classification, such as `PERSON` what the rule should be. The configuration takes the type of operation to be performed and any operations for that type of operation. For instance, if we wanted to replace each patient name with a static value, we would use the `replace` operation, and provide the `new_value` of `REDACTED`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa39cf70-b642-48f5-b343-d8d74faba462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "operators = {\n",
    "    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"REDACTED\"})\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bf86f92-205d-4bed-8733-26fdfbe9fa50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, we take our analysis and operator configuration and using the engine, we pass in the Pandas dataframe we want to anonymize, which will produce the resulting dataframe back to us, with our data changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62bcc75-d965-4be4-b465-fe11bb05aef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "anonymized_df = pandas_engine.anonymize(pandas_df, custom_analysis, operators=operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d23f21-0b0c-48e1-bee5-4a1b39a6a7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(anonymized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72299221-6088-4ecc-99fc-2689a8a9f3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Working with PySpark and Presidio\n",
    "\n",
    "Currently, PySpark isn't a supported Engine in Presidio, but it is on the roadmap. In the meantime though, we can still leverage it through UDFs on our original dataset. Since our main dataset has most of the data in the `meta` column, we can use the JSON Engine combined with Spark UDFs to work with the data as-is.\n",
    "\n",
    "We have a couple other challenges though. Firstly, the data that could contain our patient names are part of a specific key in the metadata, and that key doesn't always exist. Furthermore, the JSON engine provided by Presidio only works with primitive types, so we need to get the exact JSON key and value we want to replace, use the operator to transform it according to our rules, then put it back as a complete document.\n",
    "\n",
    "Fortunately, Spark makes this easy with Pandas UDF in Spark Dataframes: \n",
    "\n",
    "Let's look at one of the rows in our sample dataset first, in a format that's a little easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2004ee0b-d3f3-4971-810d-4b7e4a1678b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_str = '{\"00080005\": {\"vr\": \"CS\", \"Value\": [\"ISO_IR 100\"]}, \"00080008\": {\"vr\": \"CS\", \"Value\": [\"DERIVED\", \"SECONDARY\"]}, \"00080016\": {\"vr\": \"UI\", \"Value\": [\"1.2.840.10008.5.1.4.1.1.1.2\"]}, \"00080018\": {\"vr\": \"UI\", \"Value\": [\"1.2.276.0.7230010.3.1.4.1787169844.2836.1454583672.169790\"]}, \"00080020\": {\"vr\": \"DA\", \"Value\": [\"19930927\"]}, \"00080021\": {\"vr\": \"DA\", \"Value\": [\"19930927\"]}, \"00080030\": {\"vr\": \"TM\"}, \"00080050\": {\"vr\": \"SH\"}, \"00080060\": {\"vr\": \"CS\", \"Value\": [\"MG\"]}, \"00080068\": {\"vr\": \"CS\", \"Value\": [\"FOR PRESENTATION\"]}, \"00080070\": {\"vr\": \"LO\"}, \"00080090\": {\"vr\": \"PN\"}, \"00081030\": {\"vr\": \"LO\", \"Value\": [\"benign_02\"]}, \"0008103E\": {\"vr\": \"LO\", \"Value\": [\"case1327\"]}, \"00082218\": {\"vr\": \"SQ\", \"Value\": [{\"00080100\": {\"vr\": \"SH\", \"Value\": [\"T-04000\"]}, \"00080102\": {\"vr\": \"SH\", \"Value\": [\"SNM3\"]}, \"00080104\": {\"vr\": \"LO\", \"Value\": [\"BREAST\"]}}]}, \"00100010\": {\"vr\": \"PN\", \"Value\": [{\"Alphabetic\": \"0687^Patient\"}]}, \"00100020\": {\"vr\": \"LO\", \"Value\": [\"0687\"]}, \"00100030\": {\"vr\": \"DA\"}, \"00100040\": {\"vr\": \"CS\", \"Value\": [\"F\"]}, \"00101010\": {\"vr\": \"AS\", \"Value\": [\"058Y\"]}, \"00181164\": {\"vr\": \"DS\", \"Value\": [0.58391, 0.58391]}, \"00181508\": {\"vr\": \"CS\", \"Value\": [\"NONE\"]}, \"00187004\": {\"vr\": \"CS\", \"Value\": [\"FILM\"]}, \"0020000D\": {\"vr\": \"UI\", \"Value\": [\"1.2.276.0.7230010.3.1.4.1787169844.2836.1454583672.169788\"]}, \"0020000E\": {\"vr\": \"UI\", \"Value\": [\"1.2.276.0.7230010.3.1.4.1787169844.2836.1454583672.169789\"]}, \"00200010\": {\"vr\": \"SH\", \"Value\": [\"benign_02\"]}, \"00200011\": {\"vr\": \"IS\", \"Value\": [1327]}, \"00200013\": {\"vr\": \"IS\"}, \"00200020\": {\"vr\": \"CS\", \"Value\": [\"P\", \"L\"]}, \"00200062\": {\"vr\": \"CS\", \"Value\": [\"R\"]}, \"00280002\": {\"vr\": \"US\", \"Value\": [1]}, \"00280004\": {\"vr\": \"CS\", \"Value\": [\"MONOCHROME2\"]}, \"00280010\": {\"vr\": \"US\", \"Value\": [4726]}, \"00280011\": {\"vr\": \"US\", \"Value\": [2011]}, \"00280100\": {\"vr\": \"US\", \"Value\": [16]}, \"00280101\": {\"vr\": \"US\", \"Value\": [12]}, \"00280102\": {\"vr\": \"US\", \"Value\": [11]}, \"00280103\": {\"vr\": \"US\", \"Value\": [0]}, \"00280301\": {\"vr\": \"CS\", \"Value\": [\"NO\"]}, \"00281040\": {\"vr\": \"CS\", \"Value\": [\"LOG\"]}, \"00281041\": {\"vr\": \"SS\", \"Value\": [-1]}, \"00281050\": {\"vr\": \"DS\", \"Value\": [127.0]}, \"00281051\": {\"vr\": \"DS\", \"Value\": [254.0]}, \"00281052\": {\"vr\": \"DS\", \"Value\": [0.0]}, \"00281053\": {\"vr\": \"DS\", \"Value\": [1.0]}, \"00281054\": {\"vr\": \"LO\", \"Value\": [\"US\"]}, \"00282110\": {\"vr\": \"CS\", \"Value\": [\"00\"]}, \"00400318\": {\"vr\": \"CS\", \"Value\": [\"BREAST\"]}, \"00400555\": {\"vr\": \"SQ\", \"Value\": []}, \"00540220\": {\"vr\": \"SQ\", \"Value\": [{\"00080100\": {\"vr\": \"SH\", \"Value\": [\"R-10242\"]}, \"00080102\": {\"vr\": \"SH\", \"Value\": [\"SNM3\"]}, \"00080104\": {\"vr\": \"LO\", \"Value\": [\"cranio-caudal\"]}, \"00540222\": {\"vr\": \"SQ\", \"Value\": []}}]}, \"20500020\": {\"vr\": \"CS\", \"Value\": [\"IDENTITY\"]}, \"has_pixel\": true, \"hash\": \"c8515a67ea226bb47615abe7f03d3cf7c77cd1cd\", \"img_min\": 0, \"img_max\": 253, \"img_avg\": 64.04732908907904, \"img_shape_x\": 4726, \"img_shape_y\": 2011, \"file_size\": 4355734}'\n",
    "\n",
    "json_obj = loads(json_str)\n",
    "json_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdc6de15-676b-423e-97f4-4a51182f4f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The patient name in this data would normally live in the key '00100010', but this key contains another document, with its own keys and values, which may or may not be present in each record we have. To properly anonymize this data then, we'll create a new UDF that takes in the data, checks for the keys, then uses the `StructuredEngine` anonymizer to replace the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f96395-6fd1-41a0-be6e-daf6d9cecc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "anonymized_column = \"meta\" # name of column to anonymize\n",
    "anonymizer = StructuredEngine(data_processor=JsonDataProcessor())\n",
    "\n",
    "# broadcast the engines to the cluster nodes\n",
    "broadcasted_anonymizer = sc.broadcast(anonymizer)\n",
    "\n",
    "# define a pandas UDF function and a series function over it.\n",
    "def anonymize_patient_name(text: str) -> str:\n",
    "\n",
    "    json_str_orig = loads(text)\n",
    "\n",
    "    if '00100010' in json_str_orig.keys():\n",
    "        sub_json = json_str_orig[\"00100010\"]\n",
    "        if 'Value' in sub_json.keys():\n",
    "            patient_name = json_str_orig[\"00100010\"][\"Value\"][0]\n",
    "\n",
    "            custom_analysis = StructuredAnalysis(entity_mapping={\n",
    "                \"Alphabetic\":\"PERSON\"\n",
    "            })\n",
    "\n",
    "            operator = {\n",
    "                \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"REDACTED\"})\n",
    "            }\n",
    "\n",
    "            anonymizer = broadcasted_anonymizer.value\n",
    "            anonymized_complex_json = anonymizer.anonymize(patient_name, custom_analysis, operators=operators)\n",
    "\n",
    "            json_str_orig[\"00100010\"][\"Value\"][0] = anonymized_complex_json\n",
    "\n",
    "    json_string_result = dumps(json_str_orig)\n",
    "\n",
    "    return json_string_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6d4d0a4-c326-4d7f-9258-5e1ce7d738cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, we'll create our Pandas UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd763cc9-8f08-44c4-84c6-300f51b1ef45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def anonymize_json(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(anonymize_patient_name)\n",
    "\n",
    "# define a the function as pandas UDF\n",
    "anonymize = pandas_udf(anonymize_json, returnType=StringType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c850dda3-fec5-48ca-9fb2-f2b0d199438c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Then we'll read our data in from our table, and append a new column with the same data, only anonymized (for comparison purposes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f52aae6-acf7-463d-887a-158cb05ea7d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select data\n",
    "input_df = spark.read.table(table)\n",
    "\n",
    "# apply the udf\n",
    "anonymized_df = input_df.withColumn(\n",
    "    \"anonymized_meta\", anonymize(col(\"meta\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8775295-8a30-4009-a281-252f5546e9bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(anonymized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "063b0965-82e2-4206-b987-063d374c5fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Other examples: Encryption/Decryption and Pseudo-Anonymization\n",
    "\n",
    "Presidio has other built-in capabilities for dealing with sensitive data, including the ability to provide encryption and decryption. It works extremely similar to how the anonymization features work: create an operator, and provide slightly different options. Let's revisit our UDF from the previous example, and make a few simple changes: mostly, this involves also broadcasting the analysis and operator configurations outside of the UDF, since ideally you should only pass the Pandas series objects (a column in a dataframe), and the key can be different depending on which column or dataset you want to operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d0ad8c-aae9-4be8-a04c-9d443ddd5a48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crypto_key = \"WmZq4t7w!z%C&F)J\"\n",
    "\n",
    "anonymized_column = \"meta\" # name of column to anonymize\n",
    "anonymizer = StructuredEngine(data_processor=JsonDataProcessor())\n",
    "\n",
    "custom_analysis = StructuredAnalysis(entity_mapping={\n",
    "    \"Alphabetic\":\"PERSON\"\n",
    "})\n",
    "\n",
    "operator = {\n",
    "    \"PERSON\": OperatorConfig(\"encrypt\", {\"key\": crypto_key})\n",
    "}\n",
    "\n",
    "# broadcast the engines to the cluster nodes\n",
    "broadcasted_anonymizer = sc.broadcast(anonymizer)\n",
    "broadcasted_analysis = sc.broadcast(custom_analysis)\n",
    "broadcasted_operator = sc.broadcast(operator)\n",
    "\n",
    "# define a pandas UDF function and a series function over it.\n",
    "def encrypt_patient_name(text: str) -> str:\n",
    "\n",
    "    json_str_orig = loads(text)\n",
    "\n",
    "    if '00100010' in json_str_orig.keys():\n",
    "        sub_json = json_str_orig[\"00100010\"]\n",
    "        if 'Value' in sub_json.keys():\n",
    "            patient_name = json_str_orig[\"00100010\"][\"Value\"][0]\n",
    "\n",
    "            anonymizer = broadcasted_anonymizer.value\n",
    "            analysis = broadcasted_analysis.value\n",
    "            operators = broadcasted_operator.value\n",
    "\n",
    "            anonymized_complex_json = anonymizer.anonymize(patient_name, analysis, operators=operators)\n",
    "\n",
    "            json_str_orig[\"00100010\"][\"Value\"][0] = anonymized_complex_json\n",
    "\n",
    "    json_string_result = dumps(json_str_orig)\n",
    "\n",
    "    return json_string_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0697a6-4eb4-42f7-9c14-628407994081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encrypt_json(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(encrypt_patient_name)#\n",
    "\n",
    "encrypt = pandas_udf(encrypt_json, returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11591508-cf05-4184-a14e-d5aac7f595e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select data\n",
    "input_df = spark.read.table(table)\n",
    "\n",
    "# apply the udf\n",
    "encrypted = input_df.withColumn(\n",
    "    \"encrypted_meta\", encrypt(col(\"meta\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3271ad5-7b5c-4a35-9319-408f3934b00d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(encrypted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c517fb78-ce7d-48e4-988e-5cf677f5523d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pseudo-anonymization is a little harder, but Presidio provides a lot of ways to extend itself by letting you build your own operators. For instance, we can take the example class provided for incrementing a patient ID per patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124ce86b-fbb1-4655-ae19-859919e15483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import instance_counter_anonymizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e4c26eb-bfbe-480f-8ca6-d7bcb166a062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "anonymized_column = \"meta\" # name of column to anonymize\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "anonymizer.add_anonymizer(instance_counter_anonymizer.InstanceCounterAnonymizer)      \n",
    "\n",
    "entity_mapping = dict()\n",
    "\n",
    "operator = {\n",
    "    \"PERSON\": OperatorConfig(\"entity_counter\", {\"entity_mapping\": entity_mapping})\n",
    "}\n",
    "\n",
    "# broadcast the engines to the cluster nodes\n",
    "broadcasted_anonymizer = sc.broadcast(anonymizer)\n",
    "broadcasted_operator = sc.broadcast(operator)\n",
    "\n",
    "# define a pandas UDF function and a series function over it.\n",
    "def psuedo_patient_name(text: str) -> str:\n",
    "\n",
    "    json_str_orig = loads(text)\n",
    "\n",
    "    if '00100010' in json_str_orig.keys():\n",
    "        sub_json = json_str_orig[\"00100010\"]\n",
    "        if 'Value' in sub_json.keys():\n",
    "            patient_name = json_str_orig[\"00100010\"][\"Value\"][0][\"Alphabetic\"]\n",
    "\n",
    "            custom_analysis = [\n",
    "                RecognizerResult(entity_type = \"PERSON\", start=0, end=len(patient_name), score=1)\n",
    "            ]\n",
    "\n",
    "            anonymizer = broadcasted_anonymizer.value\n",
    "            operators = broadcasted_operator.value\n",
    "\n",
    "            anonymized_name = anonymizer.anonymize(patient_name, custom_analysis, operators=operators)\n",
    "\n",
    "            json_str_orig[\"00100010\"][\"Value\"][0][\"Alphabetic\"] = anonymized_name.text\n",
    "\n",
    "    json_string_result = dumps(json_str_orig)\n",
    "\n",
    "    return json_string_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a6fee9-fbbf-4399-b94a-73165d59039a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def psuedo_json(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(psuedo_patient_name)#\n",
    "\n",
    "psuedo = pandas_udf(psuedo_json, returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "092a1de9-f904-40a6-b24d-04f3188001c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select data\n",
    "input_df = spark.read.table(table)\n",
    "\n",
    "# apply the udf\n",
    "psuedo_df = input_df.withColumn(\n",
    "    \"psuedo_meta\", psuedo(col(\"meta\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2209244f-0cba-445a-9808-b0a518bc8af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(psuedo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cd521fa-2ff1-40ba-969d-d80d473d123c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### De-identifying DICOM Images\n",
    "\n",
    "Another feature of Presidio is the ability to identify data inside of images as well, using the same concepts with text: first you use an analyzer to detect any sensitive information in the image, then another engine to handle the removal of the sensitve data. Presidio has different methods for analyzing and handling the image, such as built-in support for Tesseract OCR () or external services (and is fully extensible like the other engines).\n",
    "\n",
    "Let's start with some sample images from the Presidio main repository, which has some sample images. We'll download those and load them into Pixels, then use Presidio to redact the sensitive info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db0ed68-ece5-4d3b-aaf1-8791934e1650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "presidio_volume =volume + \"_presidio_images\"    \n",
    "    \n",
    "if (spark.sql(f\"show volumes in {schema} like '{presidio_volume }'\").count() == 0):\n",
    "    spark.sql(f\"create volume if not exists {presidio_volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab32614-6017-4dd8-a409-727d22d27b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "sample_images = [\n",
    "  \"https://github.com/microsoft/presidio/raw/refs/heads/main/presidio-image-redactor/tests/test_data/0_ORIGINAL.dcm\",\n",
    "  \"https://github.com/microsoft/presidio/raw/refs/heads/main/presidio-image-redactor/tests/test_data/dicom_dir_1/dicom_dir_2/1_ORIGINAL.DCM\"\n",
    "]\n",
    "\n",
    "for s in sample_images:\n",
    "  r = requests.get(s, allow_redirects=True)\n",
    "  filename = s.rsplit('/', 1)[1]\n",
    "  open(\"/Volumes/\" + presidio_volume.replace(\".\",\"/\") + \"/\" + filename, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497bedc9-48de-4674-a901-dc9d21e6592e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = Catalog(spark, table=table+\"_presidio_images\", volume=presidio_volume)\n",
    "catalog_df = catalog.catalog(path=\"/Volumes/\" + presidio_volume.replace(\".\",\"/\") + \"/\", extractZip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344a3ccf-0689-4d02-b770-3b21cd4925ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "meta_df = DicomMetaExtractor(catalog).transform(catalog_df)\n",
    "thumbnail_df = DicomThumbnailExtractor().transform(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63b1d8e-8ba8-4a68-951b-d3464abdb197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog.save(thumbnail_df, mode=write_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f553f02b-4aa2-484d-ba04-56a4af607887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11588029-e206-4efe-9bc8-8fce2b9f0410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(thumbnail_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb56825-4440-46aa-bd7f-037187c38841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dcm_df_filtered = Catalog(spark, table=table+\"_presidio_images\", volume=presidio_volume).load().filter('lower(extension) = \"dcm\"').limit(1000)\n",
    "DicomPlot(dcm_df_filtered).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea64871-10d3-4527-b9c7-c8cd998de0f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/\" + presidio_volume.replace(\".\",\"/\") + \"/1_ORIGINAL.DCM\"\n",
    "redacted_path= \"/\".join(path.split(\"/\")[:-1]) + \"/REDACTED/\" + path.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c401d5e9-d50d-407c-8695-9323458b486c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from pyspark.sql.functions import pandas_udf, col\n",
    "#from pyspark.sql.types import StringType\n",
    "import pydicom\n",
    "from presidio_image_redactor import DicomImageRedactorEngine\n",
    "\n",
    "engine = DicomImageRedactorEngine()\n",
    "\n",
    "broadcasted_engine= sc.broadcast(engine)\n",
    "\n",
    "# define a pandas UDF function and a series function over it.\n",
    "def redact_dicom_image(path: str) -> str:\n",
    "\n",
    "    #redacted_path= \"/\".join(path.split(\"/\")[:-1]) + \"/REDACTED/\" + path.split(\"/\")[-1]\n",
    "    redacted_path= \"/\".join(path.split(\"/\")[:-1]) + \"/REDACTED/\"\n",
    "    engine = broadcasted_engine.value\n",
    "    engine.redact_from_file(path, redacted_path, padding_width=25, fill=\"contrast\")\n",
    "    return redacted_path + path.split(\"/\")[-1]\n",
    "  \n",
    "def redact_dicom(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(redact_dicom_image)#\n",
    "\n",
    "redact = pandas_udf(redact_dicom, returnType=StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc987e82-a47b-481c-a61a-ee9fa17796b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# apply the udf\n",
    "redacted_df = thumbnail_df.withColumn(\n",
    "    \"redacted_image_path\", redact(col(\"local_path\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c11c9d2a-438a-405c-beaf-6352f947c031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(redacted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36306f9-16dd-45c9-8058-7c1c8bdd5ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = Catalog(spark, table=table+\"_presidio_images_redacted\", volume=presidio_volume)\n",
    "catalog_df = catalog.catalog(path=\"/\".join(path.split(\"/\")[:-1]) + \"/REDACTED/\", extractZip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4c4526-035c-4438-a02b-79ac368365e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "meta_df = DicomMetaExtractor(catalog).transform(catalog_df)\n",
    "thumbnail_df = DicomThumbnailExtractor().transform(meta_df)\n",
    "display(thumbnail_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54dc674e-06ee-4efe-b082-1b05cb431f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DicomPlot(thumbnail_df).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1242762840048992,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "08-De-identification-With-Presidio",
   "widgets": {
    "mode": {
     "currentValue": "append",
     "nuid": "8b0d7469-a00c-4f36-989b-e0a4f31f7aee",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "append",
      "label": "4.0 Update mode on object metadata table",
      "name": "mode",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "overwrite",
        "append"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "append",
      "label": "4.0 Update mode on object metadata table",
      "name": "mode",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "overwrite",
        "append"
       ]
      }
     }
    },
    "path": {
     "currentValue": "s3://hls-eng-data-public/dicom/landing_zone/*.zip",
     "nuid": "59881fab-fa47-4233-a62d-d8f52ea28b22",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "s3://hls-eng-data-public/dicom/landing_zone/*.zip",
      "label": "1.0 Path to directory tree containing files. /dbfs , /Volumes/ or s3:// supported",
      "name": "path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "s3://hls-eng-data-public/dicom/landing_zone/*.zip",
      "label": "1.0 Path to directory tree containing files. /dbfs , /Volumes/ or s3:// supported",
      "name": "path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table": {
     "currentValue": "drewfurgiuele.pixels_solacc.object_catalog",
     "nuid": "4ef8a1e7-f2e8-4c0c-a22b-0fec986ffb5d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main.pixels_solacc.object_catalog",
      "label": "2.0 Catalog Schema Table to store object metadata into",
      "name": "table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main.pixels_solacc.object_catalog",
      "label": "2.0 Catalog Schema Table to store object metadata into",
      "name": "table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume": {
     "currentValue": "drewfurgiuele.pixels_solacc.pixels_volume",
     "nuid": "0bb07912-3bf0-43cf-9117-a9354292736c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main.pixels_solacc.pixels_volume",
      "label": "3.0 Catalog Schema Volume to store checkpoints and unzipped files",
      "name": "volume",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main.pixels_solacc.pixels_volume",
      "label": "3.0 Catalog Schema Volume to store checkpoints and unzipped files",
      "name": "volume",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
